---
title: "INFOMDA2: Group Assignment"
author: "Martijn van Dam, Jonathan Koop, Merlin Urbanski"
format: html
---

# 1. Preparations

## 1.1 Load Packages

```{r}
library(pacman)

p_load("readr", "tidyverse", "caret")
```

## 1.2 Load Data

```{r}
data <- read.csv("../raw_data/data.csv")
```


# 2. Data Exploration

First, we can explore what the data looks like by looking at the first few rows of the data set.

```{r}
head(data)
```

We can also look at the structure of the data set to see what kind of variables we are dealing with.

```{r}
str(data)
```

As can be seen from the output above, all variables except for `ID` are either numeric or integers. Since we have a categorical outcome, we have to transform it to a factor

```{r}
data <- data %>%
  mutate(class = as.factor(class)) # change class to factor
```


# 3. Benchmark Approach (Not High-Dimensional)

## 3.1 Variable/Feature Selection

In our benchmark approach, which does not properly deal with the high dimensionality of our data, we use $t$-tests for differences in group means in the dependent variable `class` for each of the independent variables.

First, we check which variables lead to a $p$-value below 0.05.

```{r}
sig_05 <- data %>%
  select(-ID) %>% # remove ID column
  summarise(across(-class, ~ t.test(.x ~ class)$p.value)) %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column("variable") %>%
  filter(V1 < 0.05)
```

The code above performs t-tests for each variable in the data set and filters out the variables for which the $p$-value is below 0.05. When we run the code, we see that this approach does not effectively reduces the number of variables, as we still have `r nrow(simple_data)` variables left. This leads to an unidentifiable model, because we still have more variables than observations. For that reason, we reduce the threshold to 0.01.

```{r}
sig_01 <- data %>%
  select(-ID) %>% # remove ID column
  summarise(across(-class, ~ t.test(.x ~ class)$p.value)) %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column("variable") %>%
  filter(V1 < 0.01)
```

We still have too many significant variables. Consequently, we reduce the threshold further.

```{r}
sig_001 <- data %>%
  select(-ID) %>% # remove ID column
  summarise(across(-class, ~ t.test(.x ~ class)$p.value)) %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column("variable") %>%
  filter(V1 < 0.001)
```


As we can see, using $p<0.001$ as a threshold reduces the number of variables to `r nrow(sig_001)`. As this is less than the number of observations, we can now proceed with the model building.


## 3.2 Model Building

### 3.2.1 Subset Data

```{r}
data_benchmark <- data %>%
  select(sig_001$variable, class) # select only significant variables and DV
```

### 3.2.2 Logistic Regression with Cross-Validation

```{r}
set.seed(123)
model_benchmark <- train(class ~ ., data = data_benchmark, method = "glm", family = "binomial", trControl = trainControl(method = "cv", number = 10))

summary(model_benchmark)
```

