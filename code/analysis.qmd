---
title: "INFOMDA2: Group Assignment"
author: "Martijn van Dam, Jonathan Koop, Merlin Urbanski"
format: html
knitr:
  opts_chunk: 
    warning: false
---

```{r}
#| echo: false
options(scipen = 999)
```


# 1. Preparations

## 1.1 Load Packages

```{r}
# if required, install pacman package
if (!requireNamespace("pacman", quietly = TRUE)) {
  install.packages("pacman")
}

library(pacman)

# load all packages with pacman
p_load("readr", "tidyverse", "caret", "kableExtra", "pROC")
```

## 1.2 Load Data

```{r}
data <- read.csv("../raw_data/data.csv") # load data
```


# 2. Data Exploration

First, we can explore what the data looks like by looking at the first few rows of the data set.

```{r}
head(data) # show first few rows of data
```

We can also look at the structure of the data set to see what kind of variables we are dealing with.

```{r}
str(data) # show structure of data
```

As can be seen from the output above, all variables except for `ID` are either numeric or integers. Since we have a categorical outcome, we have to transform it to a factor

```{r}
# Convert class to factor
data$class <- as.factor(data$class)
```


# 3. Benchmark Approach (Not High-Dimensional)

## 3.1 Variable/Feature Selection

In our benchmark approach, which does not properly deal with the high dimensionality of our data, we use $t$-tests for differences in group means in the dependent variable `class` for each of the independent variables.

First, we check which variables lead to a $p$-value below 0.05.

```{r}
sig_05 <- data %>%
  select(-ID) %>% # remove ID column
  summarise(across(-class, ~ t.test(.x ~ class)$p.value)) %>% # perform t-tests
  t() %>% # transpose
  as.data.frame() %>%
  rownames_to_column("variable") %>% # add variable names as a column
  filter(V1 < 0.05) # filter out variables with p-value < 0.05
```

The code above performs t-tests for each variable in the data set and filters out the variables for which the $p$-value is below 0.05. When we run the code, we see that this approach does not effectively reduces the number of variables, as we still have `r nrow(sig_05)` variables left. This leads to an unidentifiable model, because we still have more variables than observations. For that reason, we reduce the threshold to 0.01.

```{r}
sig_01 <- data %>%
  select(-ID) %>% # remove ID column
  summarise(across(-class, ~ t.test(.x ~ class)$p.value)) %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column("variable") %>%
  filter(V1 < 0.01) # filter out variables with p-value < 0.01
```

We still have too many significant variables. Consequently, we reduce the threshold further.

```{r}
sig_001 <- data %>%
  select(-ID) %>% # remove ID column
  summarise(across(-class, ~ t.test(.x ~ class)$p.value)) %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column("variable") %>%
  filter(V1 < 0.001) # filter out variables with p-value < 0.001
```


As we can see, using $p<0.001$ as a threshold reduces the number of variables to `r nrow(sig_001)`. As this is less than the number of observations, we can now proceed with the model building.


## 3.2 Model Building

### 3.2.1 Subset Data

```{r}
data_benchmark <- data %>%
  select(sig_001$variable, class) # select only significant variables and DV
```

### 3.2.2 Logistic Regression with Cross-Validation

#### 3.2.2.1 Training the Model

```{r}
# | warning: false 

set.seed(123) # set seed for reproducibility

# Train logistic regression model
model_benchmark <- train(
  class ~ ., # all significant variables
  data = data_benchmark,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "LOOCV", savePredictions = "final", classProbs = TRUE)) # leave-one-out cross-validation

summary(model_benchmark) # show summary of model
```

Even though we have reduced the number of variables to `r nrow(sig_001)`, the algorithm still does not seem to converge.

#### 3.2.2.2 Making Predictions and Evaluating the Model

```{r}
# Accuracy and Kappa
model_benchmark$results

# ROC Curve and AUC
predictions_model_benchmark <- model_benchmark$pred
roc_benchmark <- roc(response = predictions_model_benchmark$obs, predictor = predictions_model_benchmark$P)
```


# 4. Principal Component Regression (High-Dimensional Approach)

## 4.1 Principal Component Analysis

### 4.1.1 Identify Principal Components

```{r}
pca <- data %>%
  select(-ID, -class) %>% # remove ID and class columns
  prcomp(center = TRUE, scale. = TRUE) # perform PCA
```

### 4.1.2 Scree Plot

```{r}
screeplot(pca, type = "lines", main = "Scree Plot") # plot scree plot
```

Analyzing the scree plot, we can see that the first component explains most of the variance in the data, explaining three times as much variance as the second component. The third component explains even less variance. This suggests that we can reduce the dimensionality of the data by only using the first component, as the elbow is already at the second component.

### 4.1.3 Variances Explained

Only seeing the absolute eigenvalue of the components does not give us a good idea of how much variance each component explains. Therefore, we calculate the proportion of variance explained by each component and the cumulative proportion of variance explained.

```{r}
pca_table <- tibble(
  component = seq_along(pca$sdev), # Component numbers
  eigenvalue = pca$sdev^2 # Variances (eigenvalues)
) %>%
  mutate(
    proportion = eigenvalue / sum(eigenvalue),  # Proportion of variance explained
    cumulative = cumsum(proportion)        # Cumulative variance explained
  ) %>%
  round(., 3)
```

As can be seen in the table below, the first component explains only `r pca_table$proportion[1]` of the variance. This implies that simply using the first component would not be a good idea, as it does not explain enough variance. We can see that even the first 10 components only explain `r pca_table$cumulative[10]` of the variance. This suggests that we should use more than just the first components. Therefore, the eigenvalue greater than 1 rule may perform better in this case.

```{r}
pca_table %>%
  filter(eigenvalue > 1) %>% # filter out components with eigenvalue > 1
  kable("html") %>% # create HTML table
  kable_styling("striped", full_width = F) # add styling
```

### 4.1.4 Eigenvalue Greater Than 1 Rule

```{r}
pc_greaterthan1 <- pca_table %>% 
  filter(eigenvalue > 1) %>% # filter out components with eigenvalue > 1
  pull(component) %>% # extract component numbers
  paste0("pc", .) # create column names
```

## 4.2 Model Building

### 4.2.1 Subset Data

```{r}
data_pca <- bind_cols(
  class = data$class,  # Add class column
  as_tibble(pca$x)     # Convert PCA matrix to tibble
) %>%
  set_names(c("class", paste0("pc", 1:ncol(pca$x)))) # Rename columns
```

### 4.2.2 Logistic Regression Using Elbow Criterion

#### 4.2.2.1 Training the Model

```{r}
model_scree <- data_pca %>%
  select(pc1, class) %>%
  train(
    class ~ .,
    data = .,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "LOOCV", savePredictions = "final", classProbs = TRUE) # leave-one-out cross-validation
  )

summary(model_scree) # show summary of model
```

#### 4.2.2.2 Making Predictions and Evaluating the Model

```{r}
# Accuracy and Kappa
model_scree$results

# ROC Curve and AUC
predictions_model_scree <- model_scree$pred
roc_scree<- roc(response = predictions_model_scree$obs, predictor = predictions_model_scree$P)
```

### 4.2.3 Logistic Regression Using Eigenvalue Greater Than 1 Rule

#### 4.2.3.1 Training the Model

```{r}
model_greaterthan1 <- data_pca %>% 
  select(all_of(pc_greaterthan1), class) %>%
  train(
    class ~ .,
    data = .,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "LOOCV", savePredictions = "final", classProbs = TRUE)
  )

summary(model_greaterthan1)
```

#### 4.2.3.2 Making Predictions and Evaluating the Model

```{r}
# Accuracy and Kappa
model_greaterthan1$results

# ROC Curve and AUC
predictions_model_greaterthan1 <- model_greaterthan1$pred
roc_greaterthan1 <- roc(response = predictions_model_greaterthan1$obs, predictor = predictions_model_greaterthan1$P)
```

# 5. Plots

## 5.1 Scree Plot

```{r}
# Slice out the first 90 components
pca_table_90 <- pca_table %>% 
  slice(1:90)


ggplot(pca_table_90, aes(x = component)) +
  # Bar plot for variance explained
  geom_col(aes(y = eigenvalue), fill = "steelblue") +
  
  # Line + points for cumulative explained variance, scaled to max(eigenvalue)
  geom_line(aes(y = cumulative * max(eigenvalue)), color = "red", size = 1) +
  geom_point(aes(y = cumulative * max(eigenvalue)), color = "red", size = 2) +
  
  # Set up x-axis breaks (optional, e.g. every 5 components)
  scale_x_continuous(breaks = seq(0, 90, by = 5)) +
  
  # Primary y-axis is variance; secondary y-axis is the cumulative proportion
  scale_y_continuous(
    name = "Variance (Eigenvalue)",
    sec.axis = sec_axis(
      trans = ~ . / max(pca_table_90$eigenvalue),
      name = "Cumulative Proportion of Variance Explained"
    )
  ) +
  
  # Labels and title
  labs(
    title = "Scree Plot For Principal Components",
    x = "Principal Component"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, face = "italic"),
    legend.position = "bottom",
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 12, face = "bold")
  )

ggsave("../plots/scree_plot.png", width = 12, height = 8)
```

## 5.2 ROC Curves

```{r}
ggroc(list(
  benchmark = roc_benchmark,
  scree = roc_scree,
  greaterthan1 = roc_greaterthan1
)) +
  labs(
    title = "ROC Curves of Different Models",
    x = "1 - Specificity (False Positive Rate)",
    y = "Sensitivity (True Positive Rate)",
    color = "Model"
  ) +
  geom_abline(
    intercept = 1,
    slope = 1,
    linetype = "dashed",
    color = "gray",
    size = 0.8
  ) +
  scale_color_manual(
    values = c("steelblue", "orange", "darkgreen"),
    labels = c("Benchmark", "Scree", "Greater Than 1")
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, face = "italic"),
    legend.position = "bottom",
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 12, face = "bold")
  )

ggsave("../plots/roc_curves.png", width = 12, height = 8)
```


# 6. Iteratively Add Components

## 6.1 Estimate Logistic Regression Models

```{r}
logit_results <- rep(NA, length(pc_greaterthan1)) # to store results
logit_warnings <- rep(FALSE, length(pc_greaterthan1)) # to track warnings

for (i in seq_along(pc_greaterthan1)) {
  captured_warning <- NULL # set caputured_warning to NULL
  logit <- suppressWarnings(
    withCallingHandlers({
      # Fit logistic regression model
      logit <- data_pca %>%
        select(all_of(pc_greaterthan1[1:i]), class) %>%
        train(
          class ~ .,
          data = .,
          method = "glm",
          family = "binomial",
          trControl = trainControl(method = "LOOCV", savePredictions = "final", classProbs = TRUE)
        )
      # Store accuracy
      logit_results[i] <- logit$results$Accuracy
    }, warning = function(w) {
      captured_warning <<- w # Capture the warning
    })
  )
  
  # logit_warnings == T if warning occurred
  if (!is.null(captured_warning)) {
    logit_warnings[i] <- TRUE
    message(paste("Warning in iteration", i, ":", conditionMessage(captured_warning)))
  }
}
```

## 6.2 Plot Accuracy by Number of Components

```{r}
# Create a data frame to combine all data
plot_data <- data.frame(
  x = 1:length(pc_greaterthan1),
  y = logit_results,
  warning = ifelse(logit_warnings, "No", "Yes") # Create a factor for warnings
)

ggplot() +
  # Line for accuracy results
  geom_line(data = plot_data, aes(x = x, y = y)) +
  
  # Points with color mapped to 'warning'
  geom_point(
    data = plot_data,
    aes(x = x, y = y, color = warning), size = 3
  ) +
  
  labs(
    x = "Number of Components",
    y = "Accuracy",
    title = "Accuracy by Number of Principal Components",
    color = "Convergence" # Label for the legend
  ) +
  scale_color_manual(values = c("No" = "darkblue", "Yes" = "darkorange")) + # Custom colors
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, face = "italic"),
    legend.position = "bottom",
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 12, face = "bold")
  )

ggsave("../plots/accuracy_by_components.png", width = 12, height = 8)
```

